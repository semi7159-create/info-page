"""
menu_gemini_chatbot.py
Streamlit ì•±: Gemini ê¸°ë°˜(ì˜µì…˜) ìŒì‹ ë©”ë‰´ ì¶”ì²œ ì±—ë´‡
- ëª©ì : ì‚¬ìš©ìì˜ ê³ ë¯¼ì— ê³µê°í•˜ê³  ìµœì†Œ 2ê°œì˜ í•µì‹¬ ì •ë³´ë¥¼ ì§ˆë¬¸í•´ 2~3ê°€ì§€ ë©”ë‰´ ì œì•ˆ
- ê¸°ë³¸ ëª¨ë¸: gemini-2.0-flash (ì„ íƒ UI ì œê³µ)
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ë™ì‘ ê·œì¹™ì€ ì½”ë“œ ë‚´ì— ë°˜ì˜ë¨
- í•µì‹¬ ê¸°ëŠ¥: ëŒ€í™” íˆìŠ¤í† ë¦¬, ìµœê·¼ 6í„´ ìœ ì§€(ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ í„´ ì‚­ì œ), CSV ìë™ ê¸°ë¡(ì˜µì…˜), ë¡œê·¸ ë‹¤ìš´ë¡œë“œ, ëŒ€í™” ì´ˆê¸°í™”, ëª¨ë¸/ì„¸ì…˜ í‘œì‹œ
- ë¹„ë°€í‚¤: st.secrets['GEMINI_API_KEY'] (ì—†ìœ¼ë©´ ì„ì‹œ ì…ë ¥ UI í‘œì‹œ)
- ì‘ì„±ì ë…¸íŠ¸: ì‹¤ì œ Google Gemini ì—°ë™ ì‹œ google-generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì • í•„ìš”(ì£¼ì„ ì°¸ê³ )
"""
import streamlit as st
from datetime import datetime
import uuid
import pandas as pd
import time
import json
import os

# ---------- ì„¤ì • ----------
st.set_page_config(page_title="ë©”ë‰´ ì¶”ì²œ ì±—ë´‡ (Gemini)", page_icon="ğŸ½ï¸", layout="wide")
SESSION_ID = st.session_state.get("session_id", str(uuid.uuid4()))
if "session_id" not in st.session_state:
    st.session_state["session_id"] = SESSION_ID

# System prompt (ìš”ì²­í•˜ì‹  ê·œì¹™ ë°˜ì˜)
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë©”ë‰´ ì¶”ì²œ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.
1) ì‚¬ìš©ìê°€ ìŒì‹ ê²°ì • ê³ ë¯¼ì„ ì–¸ê¸‰í•˜ë©´ ì¦‰ì‹œ ê³µê°í•˜ê³  ë©”ë‰´ ì¶”ì²œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
2) ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ì‚¬ìš©ìì—ê²Œ ìµœì†Œ 2ê°€ì§€ ì´ìƒì˜ í•µì‹¬ ì •ë³´ë¥¼ ì§ˆë¬¸í•˜ê³  ìˆ˜ì§‘í•˜ì„¸ìš”. í•œ ë²ˆì— ë„ˆë¬´ ë§ì€ ê²ƒì„ ë¬»ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²˜ëŸ¼ í•˜ì„¸ìš”.
3) ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë©”ë‰´ 2~3ê°€ì§€ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
4) ì‚¬ìš©ìê°€ ì œì•ˆí•œ ë©”ë‰´ë¥¼ ê±°ì ˆí•˜ê±°ë‚˜ ë§ì„¤ì´ë©´ ì¦‰ì‹œ ëŒ€ì•ˆì„ ì œì‹œí•˜ê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ì„ í˜¸ë¥¼ ë‹¤ì‹œ íŒŒì•…í•˜ì„¸ìš”.
5) ì‚¬ìš©ìê°€ "ì•„ë¬´ê±°ë‚˜" ë˜ëŠ” "ì¶”ì²œí•´ ì£¼ëŠ” ê±°"ë¼ê³  ë§í•˜ë©´ ì ˆëŒ€ "ì•„ë¬´ê±°ë‚˜ìš”?"ë¼ê³  ë˜ë¬»ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  ê°€ì¥ ëŒ€ì¤‘ì ì¸ ë©”ë‰´(ì˜ˆ: ì œìœ¡ë³¶ìŒ, ëˆê¹ŒìŠ¤, ë–¡ë³¶ì´)ë¥¼ ë¨¼ì € í•˜ë‚˜ ì œì•ˆí•˜ê±°ë‚˜, ì„ íƒì§€ë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ ì¢íˆëŠ” ì§ˆë¬¸(ì˜ˆ: "ì¢‹ì•„ìš”! ê·¸ëŸ¼ ë°¥ vs ë©´, ë”± í•˜ë‚˜ë§Œ ê³¨ë¼ì£¼ì„¸ìš”!")ì„ í•˜ì„¸ìš”.
6) ë‹¹ì‹ ì€ ë©”ë‰´ ì¶”ì²œë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤. ë ˆì‹œí”¼/ì˜ì–‘/ë°°ë‹¬ ë“±ì€ 'ì €ëŠ” ë©”ë‰´ ì¶”ì²œ ì „ë¬¸ ì±—ë´‡ì´ë¼ ê·¸ ë¶€ë¶„ì€ ë„ì™€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš” ğŸ˜¥.' ë¼ê³  ì •ì¤‘íˆ ì•ˆë‚´í•˜ê³  ì¶”ì²œ ì‘ì—…ì— ì§‘ì¤‘í•˜ì„¸ìš”.
"""

# Trigger keywords (ì‚¬ìš©ì ì‹œì‘ ë¬¸êµ¬ë“¤)
TRIGGERS = ["ë­ ë¨¹ì§€", "ë©”ë‰´ ì¶”ì²œ", "ë°°ê³ íŒŒ", "ë­ ë¨¹ì„ê¹Œ", "ë­ë¨¹ì§€", "ì¶”ì²œí•´", "ì¶”ì²œí•´ì¤˜", "ì¶”ì²œí•´ ì¤˜", "ì•„ë¬´ê±°ë‚˜"]

# App UI
st.title("ğŸ½ï¸ ë©”ë‰´ ì¶”ì²œ ì±—ë´‡ (Gemini API ì‚¬ìš© ê°€ëŠ¥)")
st.markdown("ìŒì‹ ê³ ë¥´ê¸° ê·€ì°®ì„ ë•Œ! ê³µê°í•˜ê³  ì§ˆë¬¸í•œ ë’¤ 2~3ê°€ì§€ êµ¬ì²´ì  ë©”ë‰´ë¥¼ ì œì•ˆí•´ ë“œë ¤ìš”.")

# Sidebar controls
with st.sidebar:
    st.header("ì„¤ì •")
    model = st.selectbox("ëª¨ë¸ ì„ íƒ", options=["gemini-2.0-flash"], index=0)
    show_session = st.checkbox("ì„¸ì…˜/ëª¨ë¸ í‘œì‹œ", value=True)
    csv_logging = st.checkbox("ëŒ€í™” ìë™ CSV ê¸°ë¡", value=False)
    reset_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    st.markdown("---")
    st.write("Gemini API Key (ì„ íƒ)")
    # st.secrets ìš°ì„ , ì—†ìœ¼ë©´ ì…ë ¥ í•„ë“œ ì œê³µ
    gemini_key = st.secrets.get("GEMINI_API_KEY") if st.secrets else None
    if not gemini_key:
        gemini_key = st.text_input("ì„ì‹œ GEMINI_API_KEY ì…ë ¥ (ì—†ìœ¼ë©´ ëª¨ë“œ: ë¡œì»¬)", type="password")
    # small help
    st.markdown("â€» ì‹¤ì œ Gemini ì—°ë™ ì‹œ `st.secrets['GEMINI_API_KEY']`ì— í‚¤ë¥¼ ë„£ê±°ë‚˜ ì—¬ê¸° ì…ë ¥ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")

# display model/session
if show_session:
    st.info(f"ì„¸ì…˜ ID: `{st.session_state['session_id']}`  â€¢  ëª¨ë¸: `{model}`")

# Reset
if reset_btn:
    st.session_state.clear()
    # regenerate session id
    st.session_state["session_id"] = str(uuid.uuid4())
    st.experimental_rerun()

# ---------- Conversation state ----------
if "history" not in st.session_state:
    # history: list of dicts: {"role":"user"/"assistant"/"system", "content": "...", "time": ts}
    st.session_state["history"] = [{"role": "system", "content": SYSTEM_PROMPT, "time": datetime.utcnow().isoformat()}]

if "collected" not in st.session_state:
    # collected info during a recommendation flow
    st.session_state["collected"] = {}  # e.g., {"cuisine": "í•œì‹", "carb": "ë°¥"}

if "turns" not in st.session_state:
    st.session_state["turns"] = 0  # counts user->assistant exchange pairs

# helper: append to history
def append_history(role, content):
    st.session_state["history"].append({"role": role, "content": content, "time": datetime.utcnow().isoformat()})

# Logging to CSV
LOGFILE = "chat_logs.csv"
def append_log(session_id, user_msg, assistant_msg):
    row = {
        "session_id": session_id,
        "timestamp": datetime.utcnow().isoformat(),
        "user": user_msg,
        "assistant": assistant_msg
    }
    df = pd.DataFrame([row])
    if not os.path.exists(LOGFILE):
        df.to_csv(LOGFILE, index=False, encoding="utf-8-sig")
    else:
        df.to_csv(LOGFILE, mode="a", header=False, index=False, encoding="utf-8-sig")

# ---------- Simple local recommendation logic ----------
# This is the "menu engine" that, given collected info, proposes menus.
MENU_DB = {
    # cuisine: {carb: [options]}
    "í•œì‹": {
        "ë°¥": ["ê¹€ì¹˜ì°Œê°œ+ë°¥", "ì œìœ¡ë³¶ìŒ+ë°¥", "ëœì¥ì°Œê°œ+ë°¥"],
        "ë©´": ["ì¹¼êµ­ìˆ˜", "ë¹„ë¹”êµ­ìˆ˜", "ì”ì¹˜êµ­ìˆ˜"],
        "ë¶„ì‹": ["ë–¡ë³¶ì´", "ê¹€ë°¥", "ìˆœëŒ€"],
        "ê¸°íƒ€": ["ë§Œë‘", "ë³¶ìŒë°¥"]
    },
    "ì¤‘ì‹": {
        "ë°¥": ["ì§œì¥ë©´(ë°¥ëŒ€ì‹ )", "ë³¶ìŒë°¥", "íƒ•ìˆ˜ìœ¡+ë°¥"],
        "ë©´": ["ì§¬ë½•", "ìœ ë¦°ê¸°(ë©´ê³¼ í•¨ê»˜)"],
        "ë¶„ì‹": ["ì¤‘í™”ë¹„ë¹”ë©´"],
        "ê¸°íƒ€": ["ë§ˆíŒŒë‘ë¶€+ë°¥"]
    },
    "ì–‘ì‹": {
        "ë°¥": ["ìŠ¤í…Œì´í¬(ê°ìorë°¥)", "ë¦¬ì¡°ë˜"],
        "ë©´": ["í¬ë¦¼íŒŒìŠ¤íƒ€", "í† ë§ˆí† íŒŒìŠ¤íƒ€"],
        "ë¶„ì‹": ["ì¹˜ì¦ˆë²„ê±°", "í”¼ì(ì¡°ê°)"],
        "ê¸°íƒ€": ["ìƒëŸ¬ë“œ"]
    },
    "ë¶„ì‹": {
        "ë°¥": ["ê¹€ë°¥+ì£¼ë¨¹ë°¥", "ë³¶ìŒë°¥"],
        "ë©´": ["ë¼ë³¶ì´(ë©´+ë–¡)"],
        "ë¶„ì‹": ["ë–¡ë³¶ì´", "ìˆœëŒ€", "íŠ€ê¹€"],
        "ê¸°íƒ€": ["í•«ë„ê·¸"]
    },
    "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ": {
        "ê¸°íƒ€": ["ë²„ê±°", "í”„ë¼ì´+ì¹˜í‚¨", "ìƒŒë“œìœ„ì¹˜"]
    },
    "ê¸°íƒ€": {
        "ë°¥": ["ë³¶ìŒë°¥", "ë®ë°¥"],
        "ë©´": ["ë¼ë©˜", "ìš°ë™"],
        "ë¶„ì‹": ["ë–¡ë³¶ì´"],
        "ê¸°íƒ€": ["ìƒëŸ¬ë“œ"]
    }
}

def get_recommendations(collected):
    # collected may contain keys: cuisine, carb, spice
    cuisine = collected.get("cuisine", "ê¸°íƒ€")
    carb = collected.get("carb", None)
    if cuisine not in MENU_DB:
        cuisine = "ê¸°íƒ€"
    candidates = []
    if carb and carb in MENU_DB[cuisine]:
        candidates = MENU_DB[cuisine][carb]
    else:
        # fallback gather several categories
        bucket = MENU_DB[cuisine]
        for k in ["ë°¥", "ë©´", "ë¶„ì‹", "ê¸°íƒ€"]:
            if k in bucket:
                candidates += bucket[k]
    # deduplicate & pick up to 3
    unique = []
    for c in candidates:
        if c not in unique:
            unique.append(c)
    return unique[:3] if unique else ["ê¹€ì¹˜ì°Œê°œ+ë°¥", "ì œìœ¡ë³¶ìŒ+ë°¥"]

# helper: simple normalization
def normalize_text(t: str):
    return t.strip().lower()

# Detect "ì•„ë¬´ê±°ë‚˜" kind of phrases
def is_anything_like_any(t: str):
    s = normalize_text(t)
    return any(x in s for x in ["ì•„ë¬´ê±°ë‚˜", "ì¶”ì²œí•´ ì£¼ëŠ” ê±°", "ì¶”ì²œí•´ì¤˜", "ë§ˆìŒëŒ€ë¡œ", "ë§˜ëŒ€ë¡œ", "ë„ˆê°€ ê³¨ë¼"])

# Detect initial trigger
def contains_trigger(t: str):
    s = normalize_text(t)
    return any(tr in s for tr in TRIGGERS)

# ---------- (ì˜µì…˜) Gemini API wrapper with 429 retry ----------
# NOTE: This is a placeholder wrapper. For real Gemini usage, install google-generativeai
# and uncomment/adjust the client calls. The wrapper implements retry-on-429 behavior.
import requests
def call_gemini_api(prompt, model_name="gemini-2.0-flash", api_key=None, max_retries=4):
    """
    Placeholder HTTP wrapper for Gemini-like API calls.
    - If api_key is None: returns None to indicate 'no external call' (use local mode).
    - If api_key provided but real client isn't set up, this will attempt a basic REST call pattern.
    Adjust this function to your environment / official client library.
    """
    if not api_key:
        return None  # caller should fallback to local mock
    # Simple exponential backoff wrapper (for 429)
    backoff = 1
    for attempt in range(1, max_retries + 1):
        try:
            # Example generic POST â€” **MUST** be adjusted to actual Gemini endpoint & payload format.
            # The current block attempts a generic OpenAI-like call (may fail). Replace with official client.
            endpoint = f"https://api.openai.com/v1/chat/completions"  # placeholder; replace with correct Gemini endpoint
            headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
            payload = {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 512,
                "temperature": 0.8
            }
            resp = requests.post(endpoint, headers=headers, json=payload, timeout=15)
            if resp.status_code == 429:
                raise requests.exceptions.HTTPError("429")
            resp.raise_for_status()
            j = resp.json()
            # Try to extract text for OpenAI-like shape
            if "choices" in j and len(j["choices"]) > 0:
                return j["choices"][0]["message"]["content"]
            # fallback
            return j.get("result", {}).get("content", "")
        except requests.exceptions.HTTPError as e:
            if resp is not None and resp.status_code == 429:
                # retry
                time.sleep(backoff)
                backoff *= 2
                continue
            else:
                # other errors -> stop and return None
                return None
        except Exception as e:
            # network error or other
            return None
    return None

# ---------- Main chat UI ----------
col1, col2 = st.columns([3,1])
with col1:
    st.subheader("ëŒ€í™”")
    # show history in chat-like form
    chat_container = st.container()
    with chat_container:
        for item in st.session_state["history"]:
            role = item["role"]
            content = item["content"]
            t = item["time"]
            if role == "system":
                continue
            if role == "user":
                st.markdown(f"**ì‚¬ìš©ì:** {content}")
            else:
                st.markdown(f"**ì±—ë´‡:** {content}")

    # input
    user_input = st.text_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="input_box")
    submit = st.button("ì „ì†¡")

with col2:
    st.subheader("ë„ì›€ë§ / ìƒíƒœ")
    st.markdown("- ì €ëŠ” `ë©”ë‰´ ì¶”ì²œ ì „ë¬¸ ì±—ë´‡`ì…ë‹ˆë‹¤.")
    st.markdown("- ìµœì†Œ 2ê°€ì§€ ì •ë³´ë¥¼ ì§ˆë¬¸í•˜ê³  2~3ê°€ì§€ êµ¬ì²´ì  ë©”ë‰´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.")
    st.markdown("- `CSV ê¸°ë¡`ì„ ì¼œë©´ ëŒ€í™”ê°€ `chat_logs.csv`ì— ì €ì¥ë©ë‹ˆë‹¤.")
    if os.path.exists(LOGFILE):
        st.download_button("ë¡œê·¸ ë‹¤ìš´ë¡œë“œ (CSV)", data=open(LOGFILE,"rb"), file_name=LOGFILE)
    if st.button("í˜„ì¬ ëŒ€í™” ë‚´ì—­ CSVë¡œ ì €ì¥(ì¦‰ì‹œ)"):
        # dump current history into csv
        rows = []
        for h in st.session_state["history"]:
            rows.append({"session_id": st.session_state["session_id"], "timestamp": h["time"], "role": h["role"], "content": h["content"]})
        df = pd.DataFrame(rows)
        fn = f"history_dump_{st.session_state['session_id']}.csv"
        df.to_csv(fn, index=False, encoding="utf-8-sig")
        st.success(f"ì €ì¥ë¨: {fn}")
    st.markdown("---")
    st.write("ì„¸ì…˜ ì •ë³´")
    st.json({"session_id": st.session_state["session_id"], "turns": st.session_state["turns"]})

# ---------- Conversation handling logic ----------
def handle_user_message(msg):
    msg = msg.strip()
    if not msg:
        return

    append_history("user", msg)

    # quick flow: if user asks about non-menu things like ë ˆì‹œí”¼/ë°°ë‹¬, refuse politely per spec
    lower = msg.lower()
    if any(x in lower for x in ["ë ˆì‹œí”¼", "ì˜ì–‘", "ì¹¼ë¡œë¦¬", "ì˜ì–‘ì„±ë¶„", "ë°°ë‹¬", "ë°°ë‹¬í•´", "ë§›ì§‘", "ìœ„ì¹˜"]):
        reply = "ì €ëŠ” ë©”ë‰´ ì¶”ì²œ ì „ë¬¸ ì±—ë´‡ì´ë¼ ê·¸ ë¶€ë¶„ì€ ë„ì™€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš” ğŸ˜¥. í•˜ì§€ë§Œ 'ê¹€ì¹˜ì°Œê°œ'ë¡œ ê²°ì •í•˜ì‹  ê±´ ì •ë§ íƒì›”í•´ìš”!"
        append_history("assistant", reply)
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    # 1) If this is an initial trigger or we're not currently in a collecting flow, start flow
    collected = st.session_state["collected"]
    # If initial trigger in message and no collected yet, start by empathizing + ask first question
    if contains_trigger(msg) and not collected:
        reply = "ì•„~ ê·¸ ë§ˆìŒ ì¶©ë¶„íˆ ì•Œì•„ìš”! ì–´ë–¤ ê±¸ ë“œì‹œê³  ì‹¶ì€ì§€ ê°™ì´ ê³¨ë¼ë“œë¦´ê²Œìš”. ìš°ì„  í•œ ê°€ì§€ë§Œ ë¬¼ì–´ë³¼ê²Œìš”: í˜¹ì‹œ **í•œì‹ / ì¤‘ì‹ / ì–‘ì‹ / ë¶„ì‹ / íŒ¨ìŠ¤íŠ¸í‘¸ë“œ** ì¤‘ì— ëŒë¦¬ëŠ” ì¢…ë¥˜ê°€ ìˆìœ¼ì„¸ìš”? (ì—†ìœ¼ë©´ 'ìƒê´€ì—†ìŒ'ì´ë¼ê³  í•´ì£¼ì„¸ìš”)"
        append_history("assistant", reply)
        st.session_state["expecting"] = "cuisine"
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    # If user says "ì•„ë¬´ê±°ë‚˜" style
    if is_anything_like_any(msg):
        # follow rule: don't reply "ì•„ë¬´ê±°ë‚˜ìš”?" â€” propose a popular menu or force narrowing question
        reply = "ì¢‹ì•„ìš”! ê·¸ëŸ¼ ë¨¼ì € í•˜ë‚˜ ì¶”ì²œë“œë¦´ê²Œìš” â€” **ì œìœ¡ë³¶ìŒ+ë°¥**ì€ ì–´ë– ì„¸ìš”? ì•„ë‹ˆë©´ 'ë°¥ vs ë©´' ì¤‘ ë”± í•˜ë‚˜ë§Œ ê³¨ë¼ì£¼ì‹¤ë˜ìš”?"
        append_history("assistant", reply)
        # if user then picks we continue
        st.session_state["expecting"] = "confirm_any"  # special
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    # If we are expecting a specific field
    expecting = st.session_state.get("expecting", None)
    if expecting == "cuisine":
        # user answered cuisine
        # normalize and store first meaningful token
        answer = msg.strip()
        # map to known categories
        map_lower = answer.lower()
        if any(k in map_lower for k in ["í•œì‹","korean"]):
            cat = "í•œì‹"
        elif any(k in map_lower for k in ["ì¤‘ì‹","chinese"]):
            cat = "ì¤‘ì‹"
        elif any(k in map_lower for k in ["ì–‘ì‹","western","ì´íƒˆë¦¬","íŒŒìŠ¤íƒ€","ìŠ¤í…Œì´í¬"]):
            cat = "ì–‘ì‹"
        elif any(k in map_lower for k in ["ë¶„ì‹","ë–¡ë³¶ì´","ê¹€ë°¥"]):
            cat = "ë¶„ì‹"
        elif any(k in map_lower for k in ["íŒ¨ìŠ¤íŠ¸","ë²„ê±°","í”¼ì","ì¹˜í‚¨"]):
            cat = "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ"
        elif "ìƒê´€ì—†" in map_lower or "ì—†ìŒ" in map_lower:
            cat = "ê¸°íƒ€"
        else:
            cat = answer  # whatever user said

        st.session_state["collected"]["cuisine"] = cat
        # ask second question
        reply = f"ì¢‹ì•„ìš” â€” **{cat}** ìª½ì´êµ°ìš”. ê·¸ëŸ¬ë©´ í•œ ê°€ì§€ë§Œ ë” ë¬¼ì„ê²Œìš”: ì˜¤ëŠ˜ì€ **ë°¥ / ë©´ / ë¶„ì‹ / ê¸°íƒ€** ì¤‘ ë¬´ì—‡ì´ ëŒë¦¬ì„¸ìš”?"
        append_history("assistant", reply)
        st.session_state["expecting"] = "carb"
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    if expecting == "carb":
        answer = msg.strip()
        a_lower = answer.lower()
        if any(k in a_lower for k in ["ë°¥","rice"]):
            carb = "ë°¥"
        elif any(k in a_lower for k in ["ë©´","êµ­ìˆ˜","noodle","ë¼ë©´","ì¹¼êµ­ìˆ˜","ìš°ë™","íŒŒìŠ¤íƒ€","ìŠ¤íŒŒê²Œí‹°"]):
            carb = "ë©´"
        elif any(k in a_lower for k in ["ë¶„ì‹","ë–¡","ê¹€ë°¥","ë–¡ë³¶ì´"]):
            carb = "ë¶„ì‹"
        else:
            carb = "ê¸°íƒ€"
        st.session_state["collected"]["carb"] = carb

        # Now we have at least 2 pieces -> propose 2~3 menus
        recs = get_recommendations(st.session_state["collected"])
        reply = f"ê°ì‚¬í•´ìš”! ì¶”ì²œì„ ë“œë¦´ê²Œìš” â€” **{st.session_state['collected'].get('cuisine','')}, {carb}** ê¸°ì¤€ìœ¼ë¡œ ì•„ë˜ ë©”ë‰´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤:\n\n"
        for i, r in enumerate(recs, 1):
            reply += f"{i}. {r}\n"
        reply += "\në§ˆìŒì— ë“œëŠ” ë²ˆí˜¸ë‚˜ í•­ëª©ì„ ê³¨ë¼ì£¼ì„¸ìš”. ë§˜ì— ë“¤ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ë‹¤ë¥¸ ëŒ€ì•ˆì„ ë” ë“œë¦´ê²Œìš”!"
        append_history("assistant", reply)
        # reset expecting so we can get selection or rejection next
        st.session_state.pop("expecting", None)
        st.session_state["turns"] += 1
        # After giving suggestions, keep collected to allow refinements, but if conversation grows, maintain last 6 turns (handled below)
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    if expecting == "confirm_any":
        # user responded to the "ì•„ë¬´ê±°ë‚˜" prompt: they may accept the suggested popular menu or choose to narrow
        if any(x in msg.lower() for x in ["ì œìœ¡","ì¢‹ì•„","ì¢‹ì•„ìš”","ok","ê´œì°®","ì˜¤ì¼€ì´","ì˜¤ì¼€"]):
            reply = "ì¢‹ì•„ìš”! ê·¸ëŸ¼ ì œìœ¡ë³¶ìŒìœ¼ë¡œ ìµœì¢… ì¶”ì²œí• ê²Œìš” ğŸš ë§›ìˆê²Œ ë“œì„¸ìš”!"
            append_history("assistant", reply)
            st.session_state.pop("expecting", None)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return
        # if they choose to narrow by 'ë°¥' or 'ë©´'
        if any(x in msg.lower() for x in ["ë°¥","ë©´","ë¶„ì‹","ê¸°íƒ€"]):
            st.session_state["collected"]["carb"] = "ë°¥" if "ë°¥" in msg else ("ë©´" if "ë©´" in msg else "ë¶„ì‹" if "ë¶„ì‹" in msg else "ê¸°íƒ€")
            recs = get_recommendations(st.session_state["collected"])
            reply = "ì•Œê² ì–´ìš”! ê·¸ëŸ¼ ì•„ë˜ì—ì„œ ê³¨ë¼ë³´ì„¸ìš”:\n"
            for i,r in enumerate(recs,1):
                reply += f"{i}. {r}\n"
            append_history("assistant", reply)
            st.session_state.pop("expecting", None)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return
        # otherwise do fallback
        reply = "ìŒ.. ì–´ë–¤ ìŠ¤íƒ€ì¼ì„ ë” ì„ í˜¸í•˜ì‹¤ì§€(ë°¥/ë©´/ë¶„ì‹ ë“±) í•˜ë‚˜ë§Œ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ì¶”ì²œ ì¢í˜€ë“œë¦´ê²Œìš”!"
        append_history("assistant", reply)
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    # If user replies to offered menu â€“ selecting or rejecting
    # Check if they select a number or name that matches last suggestions
    # Find last assistant suggestions in history (simple search)
    last_assistant = None
    for h in reversed(st.session_state["history"]):
        if h["role"] == "assistant":
            last_assistant = h["content"]
            break

    if last_assistant and any(ch.isdigit() for ch in msg):
        # try to parse a chosen number
        chosen = None
        for token in msg.split():
            if token.isdigit():
                try:
                    n = int(token)
                    # extract list from last_assistant lines
                    lines = [ln.strip() for ln in last_assistant.splitlines() if ln.strip()]
                    opts = [ln.split(". ",1)[1] if ". " in ln else ln for ln in lines if ln[0].isdigit() or ln.startswith("1.")]
                    if 1 <= n <= len(opts):
                        chosen = opts[n-1]
                        break
                except Exception:
                    continue
        if chosen:
            reply = f"ì¢‹ì€ ì„ íƒì´ì—ìš” â€” **{chosen}**ìœ¼ë¡œ ê²°ì •í•˜ì…¨êµ°ìš”! ë§›ìˆê²Œ ë“œì„¸ìš” ğŸ˜Š"
            append_history("assistant", reply)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return

    # If user explicitly rejects proposals (ë§ì„¤ì„/ê±°ì ˆ ê°™ì€ í‚¤ì›Œë“œ)
    if any(x in lower for x in ["ì•„ë‹ˆ", "ì•„ë‹ˆìš”", "ì‹«ì–´", "ë³„ë¡œ", "ë‹¤ë¥¸", "ë‹¤ë¥¸ê±°", "ì•„ë‹˜"]):
        # Immediately propose alternatives or ask a clarifying question
        reply = "ê´œì°®ì•„ìš”, ì‹¤ë§í•˜ì§€ ì•Šì•„ìš”! ì¡°ê¸ˆ ë” ì¢í˜€ë³¼ê²Œìš” â€” ë§µê²Œ ë“œì‹¤ë˜ìš”, ì•„ë‹ˆë©´ ìˆœí•˜ê²Œ ë“œì‹¤ë˜ìš”? ë˜ëŠ” ê°€ê²©ëŒ€(ì €ë ´/ë³´í†µ/ê³ ê¸‰) ì¤‘ í•˜ë‚˜ë§Œ ê³¨ë¼ì£¼ì„¸ìš”."
        append_history("assistant", reply)
        st.session_state["expecting"] = "refine_preference"
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    if st.session_state.get("expecting") == "refine_preference":
        # try simple heuristics
        if any(x in lower for x in ["ë§µ","ë§¤ìš´"]):
            reply = "ë§µê²Œ ì›í•˜ì‹œëŠ”êµ°ìš”! ê·¸ëŸ¼ ë§¤ìš´ ì œìœ¡ë³¶ìŒ, ë§¤ìš´ ë–¡ë³¶ì´ ë“±ìœ¼ë¡œ ë°”ë¡œ ì¶”ì²œë“œë¦´ê²Œìš”:\n1. ë§¤ìš´ ì œìœ¡ë³¶ìŒ+ë°¥\n2. ë§¤ìš´ ë–¡ë³¶ì´\nì›í•˜ì‹œë©´ 1 ë˜ëŠ” 2ë¡œ ê³¨ë¼ì£¼ì„¸ìš”."
            append_history("assistant", reply)
            st.session_state.pop("expecting", None)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return
        if any(x in lower for x in ["ìˆœ","ì•ˆë§µ","ìˆœí•˜ê²Œ"]):
            reply = "ìˆœí•œ ê±¸ë¡œìš”! ê·¸ëŸ¼ ë‹¤ìŒ ì¤‘ ê³¨ë¼ë³´ì„¸ìš”:\n1. ëˆê¹ŒìŠ¤+ë°¥\n2. í¬ë¦¼ íŒŒìŠ¤íƒ€\nì›í•˜ì‹œë©´ 1 ë˜ëŠ” 2ë¡œ ê³¨ë¼ì£¼ì„¸ìš”."
            append_history("assistant", reply)
            st.session_state.pop("expecting", None)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return
        if any(x in lower for x in ["ì €ë ´","ì‹¼","ê°€ë²¼ìš´"]):
            reply = "ê°€ë²¼ìš´/ì €ë ´í•œ ì˜µì…˜ ì›í•˜ì‹œëŠ”êµ°ìš” â€” ë–¡ë³¶ì´, ê¹€ë°¥, ë¶„ì‹ ìœ„ì£¼ë¡œ ì¶”ì²œë“œë¦´ê²Œìš”:\n1. ë–¡ë³¶ì´\n2. ê¹€ë°¥\nì›í•˜ì‹œë©´ ê³¨ë¼ì£¼ì„¸ìš”."
            append_history("assistant", reply)
            st.session_state.pop("expecting", None)
            if csv_logging:
                append_log(st.session_state["session_id"], msg, reply)
            return
        # otherwise generic alternative
        reply = "ì•Œê² ì–´ìš”. ê·¸ëŸ¼ ì „í˜€ ë‹¤ë¥¸ ë¶„ìœ„ê¸°ì˜ 2ê°€ì§€ ëŒ€ì•ˆ ë“œë¦´ê²Œìš”:\n1. ì œìœ¡ë³¶ìŒ+ë°¥\n2. í¬ë¦¼íŒŒìŠ¤íƒ€\nì–´ëŠ ìª½ì´ ë” ëŒë¦¬ì„¸ìš”?"
        append_history("assistant", reply)
        st.session_state.pop("expecting", None)
        if csv_logging:
            append_log(st.session_state["session_id"], msg, reply)
        return

    # If nothing matched above, fallback: be helpful and ask a gentle clarifying Q
    reply = "ì–´ë–¤ ìŠ¤íƒ€ì¼ì„ ì›í•˜ì‹œëŠ”ì§€ í•œ ê°€ì§€ë§Œ ì•Œë ¤ì£¼ì‹¤ë˜ìš”? (ì˜ˆ: í•œì‹/ì¤‘ì‹/ì–‘ì‹/ë¶„ì‹ ë˜ëŠ” ë°¥/ë©´ ì¤‘ ì„ íƒ)"
    append_history("assistant", reply)
    if csv_logging:
        append_log(st.session_state["session_id"], msg, reply)
    st.session_state["expecting"] = "cuisine"
    return

# When user presses submit
if submit and user_input:
    handle_user_message(user_input)
    # trim history to keep "ìµœê·¼ 6í„´ ìœ ì§€ í›„ ì¬ì‹œì‘" -> interpret as keep the most recent 6 user-assistant pairs
    # We'll count pairs roughly: each pair is two messages; so 12 messages + system at start ~ keep last 13 entries
    # Simpler: keep system + last 12 messages (6 turns)
    MAX_MESSAGES = 1 + 12
    if len(st.session_state["history"]) > MAX_MESSAGES:
        # keep system + last 12 entries
        system = st.session_state["history"][0]
        tail = st.session_state["history"][-12:]
        st.session_state["history"] = [system] + tail
    # re-run to show updated chat
    st.experimental_rerun()
